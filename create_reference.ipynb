{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Add necessary paths for custom modules\n",
    "sys.path.append(\"./sdxl-unbox\")\n",
    "\n",
    "from SDLens import HookedStableDiffusionXLPipeline\n",
    "\n",
    "# Grounded SAM2 and Grounding DINO imports\n",
    "sys.path.append(\"./Grounded-SAM-2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model\n",
    "from gsam_utils import sam_mask, resize_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "n_steps = 4\n",
    "use_down = True\n",
    "use_up = True\n",
    "use_up0 = True\n",
    "use_mid = True\n",
    "n_examples_per_edit = 50\n",
    "prefix = './results'\n",
    "dtype = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dtype == \"float16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = HookedStableDiffusionXLPipeline.from_pretrained(\n",
    "    'stabilityai/sdxl-turbo',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "if dtype == torch.float32:\n",
    "    pipe.text_encoder_2.to(dtype=dtype)\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM2_CHECKPOINT = \"./Grounded-SAM-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"./Grounded-SAM-2/grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"./Grounded-SAM-2/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "sam2_model = build_sam2(SAM2_MODEL_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_images(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe=pipe, \n",
    "         n_steps=1, verbose=False,\n",
    "         sam_predictor=sam2_predictor, grounding_model=grounding_model, \n",
    "         result_name=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)    \n",
    "    \n",
    "    seed = 42\n",
    "    base_imgs1, cache1 = pipe.run_with_cache(\n",
    "        prompt1,\n",
    "        positions_to_cache=[],\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    base_imgs2, cache2 = pipe.run_with_cache(\n",
    "        prompt2,\n",
    "        positions_to_cache=[],\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    img1 = base_imgs1[0][0]\n",
    "    img2 = base_imgs2[0][0]\n",
    "\n",
    "    # save to path\n",
    "    img1.save(f\"{result_name}_img1.png\")\n",
    "    img2.save(f\"{result_name}_img2.png\")\n",
    "\n",
    "    if gsam_prompt1 == \"#everything\":\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask1 = np.logical_not(mask1)\n",
    "            if verbose:\n",
    "                plt.imshow(mask1)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if gsam_prompt2 == \"#everything\":\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask2 = np.logical_not(mask2)\n",
    "            if verbose:\n",
    "                plt.imshow(mask2)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if mask1.sum() == 0 or mask2.sum() == 0:\n",
    "        raise ValueError(\"one of the masks is empty\")\n",
    "\n",
    "    # also save the detections objects and labels 2 and annotated frames using pickle\n",
    "    with open(f\"{result_name}_detections1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(detections1, f)\n",
    "    with open(f\"{result_name}_detections2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(detections2, f)\n",
    "    with open(f\"{result_name}_labels1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(labels1, f)\n",
    "    with open(f\"{result_name}_labels2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(labels2, f)\n",
    "    with open(f\"{result_name}_annotated_frame1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(annotated_frame1, f)\n",
    "    with open(f\"{result_name}_annotated_frame2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(annotated_frame2, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "from collections import defaultdict\n",
    "with open(\"./dataset/riebench.json\", \"r\") as f:\n",
    "    rb = json.load(f)\n",
    "\n",
    "\n",
    "def remove_brakets(txt):\n",
    "     return txt.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "cnt = defaultdict(int)\n",
    "\n",
    "for d in rb:\n",
    "     try:\n",
    "          if d[\"editing_type_id\"] in [] or cnt[d[\"editing_type_id\"]] >= n_examples_per_edit:\n",
    "               continue\n",
    "          key = d[\"id\"]\n",
    "          print(key)\n",
    "          path = os.path.join(prefix, f\"reference/{d['editing_type_id']}\")\n",
    "          original_prompt = remove_brakets(d[\"original_prompt\"])\n",
    "          editing_prompt = remove_brakets(d[\"editing_prompt\"])\n",
    "          os.makedirs(path, exist_ok=True)\n",
    "          if d[\"editing_type_id\"] in ['0']:\n",
    "               continue\n",
    "          else:\n",
    "               create_reference_images(editing_prompt, original_prompt, d[\"editing_gsam_prompt\"], d[\"original_gsam_prompt\"], result_name=f\"{path}/{key}\", n_steps=n_steps)\n",
    "          cnt[d[\"editing_type_id\"]] += 1\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riebench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
