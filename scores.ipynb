{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from torch import rand\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import PIL\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "path = \"./results/modesae_spatialTrue_subtractTrue_downTrue_upTrue_up0True_midTrue_T4_ktrans80_str2.0\"\n",
    "name = \"sae_80_2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPScorer:\n",
    "    def __init__(self, model_name='ViT-L-14'):\n",
    "        self.model_name = model_name\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n",
    "        self.model.cuda()\n",
    "        self.model.eval()\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "    def embed_texts(self, texts):\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            text = self.tokenizer(texts).cuda()\n",
    "            text_features = self.model.encode_text(text)\n",
    "        return text_features\n",
    "\n",
    "    def embed_images(self, images):\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            tensors = []\n",
    "            for img in images:\n",
    "                if isinstance(img, np.ndarray):\n",
    "                    img = Image.fromarray(img)\n",
    "                tensor = self.preprocess(img).unsqueeze(0)\n",
    "                tensors += [tensor]\n",
    "            tensors = torch.cat(tensors, dim=0)\n",
    "            image_features = self.model.encode_image(tensors.cuda())\n",
    "        return image_features\n",
    "\n",
    "    def get_scores(self, texts, images, normalize=True):\n",
    "        text_features = self.embed_texts(texts)\n",
    "        image_features = self.embed_images(images)\n",
    "        if normalize:\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        scores = (text_features @ image_features.T)\n",
    "        return scores\n",
    "\n",
    "    def get_scores_images(self, images1, images2, normalize=True):\n",
    "        image_features1 = self.embed_images(images1)\n",
    "        image_features2 = self.embed_images(images2)\n",
    "        if normalize:\n",
    "            image_features1 /= image_features1.norm(dim=-1, keepdim=True)\n",
    "            image_features2 /= image_features2.norm(dim=-1, keepdim=True)\n",
    "        scores = (image_features1 @ image_features2.T)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2lpips(img): # resulve this RuntimeError: Could not infer dtype of PngImageFile\n",
    "    if isinstance(img, PIL.Image.Image):\n",
    "        # this is what i tried to do    \n",
    "        img = np.array(img)\n",
    "        return (torch.tensor(img).float()/255.).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    if isinstance(img, np.ndarray) and img.dtype == np.uint8:\n",
    "        return (torch.tensor(img).float()/255.).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = \"./results/reference\"\n",
    "# pre load all of the reference images\n",
    "ref_images = {}\n",
    "\n",
    "# Process subfolders 1-9\n",
    "for i in range(1, 10):\n",
    "    subfolder_path = os.path.join(ref_path, str(i))\n",
    "    if os.path.exists(subfolder_path):\n",
    "        for ref in glob.glob(os.path.join(subfolder_path, \"*_img1.png\")):\n",
    "            base_name = os.path.basename(ref).replace(\"_img1.png\", \"\")\n",
    "            img1_path = ref\n",
    "            img2_path = ref.replace(\"_img1.png\", \"_img2.png\")\n",
    "            #print(img1_path, img2_path)\n",
    "            if os.path.exists(img2_path):\n",
    "                ref_images[f\"{i}/{base_name}_img1\"] = Image.open(img1_path)\n",
    "                ref_images[f\"{i}/{base_name}_img2\"] = Image.open(img2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_scorer = CLIPScorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"./dataset/riebench.json\", \"r\"))\n",
    "id2data = {d[\"id\"]:d for d in dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_path(name, path, ref_images, n_imgs = None, allowed_tasks=None):\n",
    "    scores = defaultdict(list)\n",
    "    # iterate over all images and calculate the score folder structure is agian 1/... 2/... etc\n",
    "    cnt = 0\n",
    "    for i in range(1, 10):\n",
    "        if allowed_tasks is not None and i not in allowed_tasks:\n",
    "            continue\n",
    "        subfolder_path = os.path.join(path, str(i))\n",
    "        if os.path.exists(subfolder_path):\n",
    "            for img in glob.glob(os.path.join(subfolder_path, \"*.png\")):\n",
    "                # select the image that does not contain _ in its name\n",
    "                if \"_\" not in os.path.basename(img):\n",
    "                    try:\n",
    "                        intervention_image = Image.open(img)\n",
    "                        # get the base name\n",
    "                        base_name = os.path.basename(img).replace(\".png\", \"\")\n",
    "                        print(\"processing\", base_name,\"...\")\n",
    "                        # get the reference images\n",
    "                        ref_image1 = ref_images[f\"{i}/{base_name}_img1\"] # edited prompt\n",
    "                        ref_image2 = ref_images[f\"{i}/{base_name}_img2\"] # original prompt\n",
    "                        intervention_jpg = intervention_image.convert('RGB')\n",
    "                        ref_image1_jpg = ref_image1.convert('RGB')\n",
    "                        ref_image2_jpg = ref_image2.convert('RGB')\n",
    "                        # calculate the score\n",
    "                        lpips_original = lpips(img2lpips(intervention_image), img2lpips(ref_image2))\n",
    "                        lpips_edited = lpips(img2lpips(intervention_image), img2lpips(ref_image1))\n",
    "                        # Convert PIL PngImageFile to jpg format in memory\n",
    "                        \n",
    "                        clip_img_original = clip_scorer.get_scores_images([intervention_jpg], [ref_image2_jpg])\n",
    "                        clip_img_edited = clip_scorer.get_scores_images([intervention_jpg], [ref_image1_jpg])\n",
    "\n",
    "                        original_prompt = id2data[base_name][\"original_prompt\"]\n",
    "                        edit_prompt = id2data[base_name][\"editing_prompt\"]\n",
    "\n",
    "                        clip_txt_original = clip_scorer.get_scores([original_prompt], [intervention_jpg])\n",
    "                        clip_txt_edited = clip_scorer.get_scores([edit_prompt], [intervention_jpg])\n",
    "                        # calculate the score\n",
    "                        scores[\"name\"] += [name]\n",
    "                        scores[\"img\"] += [\"base_name\"]\n",
    "                        scores[\"lpips_original\"] += [lpips_original.item()]\n",
    "                        scores[\"lpips_edited\"] += [lpips_edited.item()]\n",
    "                        scores[\"clip_img_original\"] += [clip_img_original.item()]\n",
    "                        scores[\"clip_img_edited\"] += [clip_img_edited.item()]\n",
    "                        scores[\"clip_txt_original\"] += [clip_txt_original.item()]\n",
    "                        scores[\"clip_txt_edited\"] += [clip_txt_edited.item()]\n",
    "                        scores[\"editing_type_id\"] += [i]\n",
    "\n",
    "                        #print(\"lpips\", lpips_original, lpips_edited)\n",
    "                        #print(\"clip\", clip_img_original, clip_img_edited)\n",
    "                        cnt += 1\n",
    "                        if n_imgs is not None and cnt >= n_imgs:\n",
    "                            return pd.DataFrame(scores)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {img}: {e}\")\n",
    "                        continue\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_path(name, path, ref_images, n_imgs = None)\n",
    "scores.to_csv(f\"{path}/{name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riebench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
